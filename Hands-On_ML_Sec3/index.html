<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Times+New+Roman:300,300italic,400,400italic,700,700italic%7CGeorgia:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hwchai.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.21.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Geron教授所著的该书第一章中已经简要介绍了监督学习任务是回归（预测数值）和分类（预测类别）。在第二章中探索了一个预测加州地区房价的回归任务，并测试了如线性回归、决策树和随机森林等算法。现在我们将注意力转向分类系统。">
<meta property="og:type" content="article">
<meta property="og:title" content="《Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow, Third Edition》 全书第三章：分类">
<meta property="og:url" content="https://hwchai.com/Hands-On_ML_Sec3/index.html">
<meta property="og:site_name" content="Hai-Wei Chai&#39;s Blog">
<meta property="og:description" content="Geron教授所著的该书第一章中已经简要介绍了监督学习任务是回归（预测数值）和分类（预测类别）。在第二章中探索了一个预测加州地区房价的回归任务，并测试了如线性回归、决策树和随机森林等算法。现在我们将注意力转向分类系统。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s21.ax1x.com/2025/01/11/pEPnQhV.jpg">
<meta property="og:image" content="https://s21.ax1x.com/2025/01/11/pEPMB90.jpg">
<meta property="og:image" content="https://s21.ax1x.com/2025/01/11/pEPQPbQ.jpg">
<meta property="og:image" content="https://s21.ax1x.com/2025/01/11/pEPQkUs.jpg">
<meta property="og:image" content="https://s21.ax1x.com/2025/01/13/pEPbgLd.png">
<meta property="og:image" content="https://s21.ax1x.com/2025/01/16/pEFyTEQ.png">
<meta property="article:published_time" content="2025-01-10T14:36:49.000Z">
<meta property="article:modified_time" content="2024-01-10T22:17:24.000Z">
<meta property="article:author" content="Hai-Wei Chai (柴海伟)">
<meta property="article:tag" content="Artificial Intelligence">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s21.ax1x.com/2025/01/11/pEPnQhV.jpg">


<link rel="canonical" href="https://hwchai.com/Hands-On_ML_Sec3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://hwchai.com/Hands-On_ML_Sec3/","path":"Hands-On_ML_Sec3/","title":"《Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow, Third Edition》 全书第三章：分类"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow, Third Edition》 全书第三章：分类 | Hai-Wei Chai's Blog</title>
  







<link rel="dns-prefetch" href="https://vercel.hwchai.com/">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hai-Wei Chai's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-house-chimney fa-fw"></i>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-list fa-fw"></i>Archives</a></li><li class="menu-item menu-item-support"><a href="/support/" rel="section"><i class="fa fa-screwdriver-wrench fa-fw"></i>Support</a></li><li class="menu-item menu-item-books"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>Books</a></li><li class="menu-item menu-item-scholar"><a href="/scholar/" rel="section"><i class="fa fa-chart-column fa-fw"></i>Scholar</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#MNIST-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.</span> <span class="nav-text">MNIST 数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">2.</span> <span class="nav-text">训练二元分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0"><span class="nav-number">3.</span> <span class="nav-text">模型性能评估</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E6%B5%8B%E9%87%8F%E7%B2%BE%E5%BA%A6"><span class="nav-number">3.1.</span> <span class="nav-text">使用交叉验证测量精度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5%EF%BC%88Confusion-Matrix-CM%EF%BC%89"><span class="nav-number">3.2.</span> <span class="nav-text">混淆矩阵（Confusion Matrix, CM）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87"><span class="nav-number">3.3.</span> <span class="nav-text">准确率和召回率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B2%BE%E7%A1%AE%E5%BA%A6-%E5%8F%AC%E5%9B%9E%E7%8E%87%E6%9D%83%E8%A1%A1%EF%BC%88precision-recall-trade-off%EF%BC%89"><span class="nav-number">3.4.</span> <span class="nav-text">精确度&#x2F;召回率权衡（precision&#x2F;recall trade-off）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%97%E8%AF%95%E8%80%85%E6%93%8D%E4%BD%9C%E7%89%B9%E5%BE%81%EF%BC%88ROC%EF%BC%89%E6%9B%B2%E7%BA%BF"><span class="nav-number">3.5.</span> <span class="nav-text">受试者操作特征（ROC）曲线</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB"><span class="nav-number">4.</span> <span class="nav-text">多元分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%94%99%E8%AF%AF%E5%88%86%E6%9E%90"><span class="nav-number">5.</span> <span class="nav-text">错误分析</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hai-Wei Chai (柴海伟)"
      src="/images/gamersky.gif">
  <p class="site-author-name" itemprop="name">Hai-Wei Chai (柴海伟)</p>
  <div class="site-description" itemprop="description">I am a slow walker, but never backwards!</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/haiweichai" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;haiweichai" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chw9402@mail.ustc.edu.com" title="E-Mail → mailto:chw9402@mail.ustc.edu.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hwchai.com/Hands-On_ML_Sec3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gamersky.gif">
      <meta itemprop="name" content="Hai-Wei Chai (柴海伟)">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hai-Wei Chai's Blog">
      <meta itemprop="description" content="I am a slow walker, but never backwards!">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow, Third Edition》 全书第三章：分类 | Hai-Wei Chai's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow, Third Edition》 全书第三章：分类
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-01-10 14:36:49" itemprop="dateCreated datePublished" datetime="2025-01-10T14:36:49Z">2025-01-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-01-10 22:17:24" itemprop="dateModified" datetime="2024-01-10T22:17:24Z">2024-01-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline: </span>
  
    <a title="waline" href="/Hands-On_ML_Sec3/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/Hands-On_ML_Sec3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>Geron教授所著的该书第一章中已经简要介绍了监督学习任务是回归（预测数值）和分类（预测类别）。在第二章中探索了一个预测加州地区房价的回归任务，并测试了如线性回归、决策树和随机森林等算法。现在我们将注意力转向分类系统。</p>
<span id="more"></span>

<h2 id="MNIST-数据集"><a href="#MNIST-数据集" class="headerlink" title="MNIST 数据集"></a>MNIST 数据集</h2><p>MNIST 数据集是一组美国高中生及人口普查局员工手写的70,000个数字图像。每个图像都标有它代表的数字。这个集合已经在机器学习领域被大量研究以至于通常称之为机器学习的“hello world”。以下代码为使用 Scikit-Learn 获取 MNIST 数据集的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_openml</span><br><span class="line">mnist = fetch_openml(<span class="string">&#x27;mnist_784&#x27;</span>,as_frame=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>sklearn.datasets 包主要包含三种类型的函数：fetch_*函数（例如fetch_openml()）用来下载现实生活中的数据集；load_*函数用来加载与Scikit-Learn捆绑的本地微型数据集；make_*函数用于生成测试数据集。生成的数据集通常包含输入数据和目标分类的元组(X,y),两者均为NumPy数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = mnist.data, mnist.target</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X.shape</span><br><span class="line">(<span class="number">70000</span>, <span class="number">784</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">array([<span class="string">&#x27;5&#x27;</span>, <span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, ..., <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;5&#x27;</span>, <span class="string">&#x27;6&#x27;</span>], dtype=<span class="built_in">object</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.shape</span><br><span class="line">(<span class="number">70000</span>,)</span><br></pre></td></tr></table></figure>

<p>共70000张图片，每张图片有784&#x3D;28*28个像素。使用 Matplotlib 的 imshow() 函数，令 cmap&#x3D;“binary” 来获取灰度颜色图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_digit</span>(<span class="params">image_data</span>):</span><br><span class="line">    image = image_data.reshape(<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">    plt.imshow(image,cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line">some_digit = X[<span class="number">0</span>]</span><br><span class="line">plot_digit(some_digit)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>这看起来像一个5，实际上标签的结果也印证了这一点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y[<span class="number">0</span>]</span><br><span class="line"><span class="string">&#x27;5&#x27;</span></span><br></pre></td></tr></table></figure>

<p>分离训练集与测试集：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]</span><br></pre></td></tr></table></figure>

<h2 id="训练二元分类器"><a href="#训练二元分类器" class="headerlink" title="训练二元分类器"></a>训练二元分类器</h2><p>现在让我们简化这个问题，只尝试识别一个数字——例如，数字5。这个“5检测器”将是二元分类器的一个例子，能够区分两个类别，5和非5。首先，我们将为这个分类任务创建目标向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_5 = (y_train == <span class="string">&#x27;5&#x27;</span>)</span><br><span class="line">y_test_5 = (y_test == <span class="string">&#x27;5&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>现在让我们选择一个分类器并训练它。一个好的起点是使用ScikitLearn的随机梯度下降（Stochastic Gradient Descent, SGD）分类器，SGDClassifier类。这个分类器能够有效地处理非常大的数据集。这部分是因为SGD独立地处理训练实例，每次处理一个，这也使得SGD非常适合在线学习。让我们创建一个SGDClassifier并在整个训练集上训练它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line">sgd_clf = SGDClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">sgd_clf.fit(X_train, y_train_5)</span><br></pre></td></tr></table></figure>

<p>现在我们可以用它来检测数字5的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.predict([some_digit])</span><br><span class="line">array([ <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<p>分类器猜测该图像代表5（True）。看起来它在这个特殊情况下猜对了！现在，我们来评估一下这个模型的性能。</p>
<h2 id="模型性能评估"><a href="#模型性能评估" class="headerlink" title="模型性能评估"></a>模型性能评估</h2><p>评估分类器通常比评估回归器要棘手得多，所以我们将在本章的大部分时间里讨论这个话题。有许多可用的绩效衡量标准：</p>
<h3 id="使用交叉验证测量精度"><a href="#使用交叉验证测量精度" class="headerlink" title="使用交叉验证测量精度"></a>使用交叉验证测量精度</h3><p>评估模型的一个好方法是使用交叉验证函数 cross_val_score() 来评估我们的 SGDClassifier 模型，使用三次的 k-fold 交叉验证。k-fold 交叉验证意味着将训练集分成k个折叠（在本例中为3个），然后训练模型 k 次，每次进行不同的折叠进行评估：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, scoring=<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">array([<span class="number">0.95035</span>, <span class="number">0.96035</span>, <span class="number">0.9604</span>])</span><br></pre></td></tr></table></figure>

<p>所有交叉验证的准确率（预测正确的比率）都在95%以上？这看起来很神奇，不是吗？好吧，在你太兴奋之前，让我们看看一个虚拟分类器，它只对最常见的类中的每个图像进行分类，在这种情况下是负类（即非5）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.dummy <span class="keyword">import</span> DummyClassifier</span><br><span class="line">dummy_clf = DummyClassifier()</span><br><span class="line">dummy_clf.fit(X_train, y_train_5)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">any</span>(dummy_clf.predict(X_train))) <span class="comment"># prints False: no 5s detected_</span></span><br></pre></td></tr></table></figure>

<p>你能猜出这个模型的精度吗？让我们来看看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(dummy_clf, X_train, y_train_5, cv=<span class="number">3</span>, scoring=<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">array([<span class="number">0.90965</span>, <span class="number">0.90965</span>, <span class="number">0.90965</span>])</span><br></pre></td></tr></table></figure>

<p>没错，准确率超过90%！这很简单，因为只有大约10%的图像是5，所以如果你总是猜测图像不是5，你将有90%的时间是正确的。这说明了为什么准确率通常不是分类器的首选性能度量，特别是当您处理倾斜的数据集。评估分类器性能的更好方法是查看混淆矩阵。</p>
<h3 id="混淆矩阵（Confusion-Matrix-CM）"><a href="#混淆矩阵（Confusion-Matrix-CM）" class="headerlink" title="混淆矩阵（Confusion Matrix, CM）"></a>混淆矩阵（Confusion Matrix, CM）</h3><p>混淆矩阵的一般思想是对于所有A&#x2F;B对计算A类实例被分类为B类的次数。例如，要知道分类器混淆8和0图像的次数，您可以查看混淆矩阵的第8行第0列。要计算混淆矩阵，首先需要有一组预测，以便将它们与实际目标进行比较。您可以对测试集进行预测，但是现在最好不要碰它（请记住，您只希望在项目的最后使用测试集，一旦您有了准备启动的分类器）。相反，你可以使用 cross_val_predict() 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict</span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p>就像 cross_val_score（）函数一样，cross_val_predict（）执行k-fold交叉验证，但它不是返回评估分数，而是返回对每个测试 fold 所做的预测。这意味着您可以对训练集中的每个实例进行干净的预测（这里的“干净”是指“样本外”：模型对训练期间从未见过的数据进行预测）。现在可以使用 confusion_matrix() 函数获得混淆矩阵了。只需将目标类 y_train_5 和预测类 y_train_pred 传递给它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix </span><br><span class="line">&gt;&gt;&gt;cm = confusion_matrix(y_train_5, y_train_pred) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cm</span><br><span class="line">array([[<span class="number">53892</span>, <span class="number">687</span>],</span><br><span class="line">	[<span class="number">1891</span>, <span class="number">3530</span>]])</span><br></pre></td></tr></table></figure>

<p>混淆矩阵中的每一行表示一个实际的类，而每一列表示一个预测的类。该矩阵的第一行考虑非5图像（阴性类）：其中53,892张被正确分类为非5（称为真阴性），而其余687张被错误分类为5（假阳性，也称为I型错误）。第二行考虑5的图像（阳性类）：1,891被错误地分类为非5（假阴性，也称为II型错误），而其余3,530被正确分类为5（真阳性）。一个完美的分类器只会有真正和真负，所以它的混淆矩阵只会在它的主对角线上（从左上到右下）有非零值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_train_perfect_predictions = y_train_5 <span class="comment"># pretend we reached perfection </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>confusion_matrix(y_train_5, y_train_perfect_predictions) </span><br><span class="line">array([[<span class="number">54579</span>, <span class="number">0</span>],</span><br><span class="line">	[ <span class="number">0</span>, <span class="number">5421</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="准确率和召回率"><a href="#准确率和召回率" class="headerlink" title="准确率和召回率"></a>准确率和召回率</h3><p>混淆矩阵为您提供了大量信息，但有时您可能更喜欢更简洁的度量。一个有趣的问题是阳性预测的准确性；这被称为分类器的准确率（Precision）。</p>
<p>$$ {\rm Precision} &#x3D; \frac{\rm TP}{\rm TP+FP} \tag{3-1}\label{3-1}$$</p>
<p>其中，TP 表示真阳性数，FP 表示假阳性数。</p>
<p>获得完美准确率的一个简单方法是创建一个分类器，它总是做出负面预测，除了对它最自信的实例进行一个单一的正面预测。如果这一个预测是正确的，那么分类器有100%的精度（精度&#x3D; 1&#x2F;1 &#x3D; 100%）。显然，这样的分类器不是很有用，因为它会忽略除一个阳性实例外的所有实例。因此，精度通常与另一个名为召回率的指标一起使用，也称为灵敏度或真阳性率（True Position Rate, TPR）：这是分类器正确检测到的阳性实例的比率。</p>
<p>$$ {\rm Recall} &#x3D; \frac{\rm TP}{\rm TP+FN} \tag{3-2}\label{3-2}$$</p>
<p>其中，FN 是假阴性的数量。</p>
<p><figure><img src="https://s21.ax1x.com/2025/01/11/pEPnQhV.jpg" alt="图 3-3: 图示的混淆矩阵显示了真阴性（左上）、假阳性（右上）、假阴性（左下）和真阳性（右下）的示例。
"><figcaption aria-hidden="true">图 3-3: 图示的混淆矩阵显示了真阴性（左上）、假阳性（右上）、假阴性（左下）和真阳性（右下）的示例。
</figcaption></figure></p>
<blockquote class="blockquote-center">
<p>简单的说，准确率代表用户得到模型判断为真的样本中确实是真的比率，召回率代表确实为真的样本被模型判断为真的比率。准确率与召回率均较高代表模型达到了优秀的性能，只有单一指标优秀不代表模型性能好。 </p>

</blockquote>

<p>Scikit-Learn 提供了几个函数来计算分类器指标，包括准确率和召回率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>precision_score(y_train_5, y_train_pred) <span class="comment"># == 3530/ (687 + 3530)</span></span><br><span class="line"><span class="number">0.8370879772350012</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recall_score(y_train_5, y_train_pred) <span class="comment"># == 3530 /(1891 + 3530)</span></span><br><span class="line"><span class="number">0.6511713705958311</span></span><br></pre></td></tr></table></figure>

<p>现在我们的5检测器看起来不像我们在考虑它的准确率时那么闪亮了。当它声称图像代表5时，正确率只有83.7%。此外，它只能检测到65.1%的5。将准确率和召回率组合成一个称为 ${\rm F}_1$ 分数的指标通常很方便，特别是当您需要一个指标来比较两个分类器时。F1分数是准确率和召回率的调和平均值。常规均值对所有值一视同仁，而调和均值对低值给予更多的权重。因此，只有在召回率和准确率都很高的情况下，分类器才会得到很高的${\rm F}_1$分数。</p>
<p>$$ {\rm F}_1 &#x3D; \frac{2}{\frac{1}{\rm Precision}+\frac{1}{\rm Recall}} &#x3D; \frac{\rm TP}{\rm TP+\frac{FN+FP}{2}} \tag{3-3}\label{3-3} $$</p>
<p>要计算 ${\rm F}_1$分数只需调用 f1_score() 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f1_score(y_train_5, y_train_pred)</span><br><span class="line"><span class="number">0.7325171197343846</span></span><br></pre></td></tr></table></figure>

<p>F1分数倾向于具有相似精度和召回率的分类器。这并不总是你想要的：在某些情况下，你最关心的是准确性，而在其他情况下，你真正关心的是召回率。例如，如果你训练一个分类器来检测对孩子安全的视频，你可能更喜欢一个分类器，它会拒绝许多好的视频（低召回率），但只保留安全的视频（高精度），而不是一个具有更高召回率的分类器，但会让一些非常糟糕的视频出现在你的产品中（在这种情况下，你甚至可能想要添加一个人工管道来检查分类器的视频选择）。另一方面，假设你训练一个分类器来检测监视图像中的商店扒手：如果你的分类器只有30%的准确率，只要它有99%的召回率，这可能是好的（当然，保安会得到一些错误的警报，但几乎所有的商店扒手都会被抓住）。不幸的是，你不能两全其美：提高精确度会降低召回率，反之亦然。这被称为精确度&#x2F;召回率权衡（precision&#x2F;recall trade-off）。</p>
<h3 id="精确度-召回率权衡（precision-recall-trade-off）"><a href="#精确度-召回率权衡（precision-recall-trade-off）" class="headerlink" title="精确度&#x2F;召回率权衡（precision&#x2F;recall trade-off）"></a>精确度&#x2F;召回率权衡（precision&#x2F;recall trade-off）</h3><p>为了理解这种权衡，让我们看看 SGDClassifier 是如何做出分类决策的。对于每个实例，它根据决策函数计算一个分数。如果该分数大于阈值，则将实例分配给正类；否则它会把它赋值给负类。图 3-4 给出了分数从左边最低到右边最高的几个数字。假设决策阈值位于中间的箭头（在两个5之间）：你会发现在该值的右边有4个真阳性，和1个假阳性。但在6个真实的5中,分类器只检测到4个。如果你提高阈值,假阳性（6）成为一个真正的阴性，从而增加的精度(在本例中高达100%)，但一个真正的5的成为假阴性，召回率降低到50%。相反，降低阈值会增加召回率，降低准确率。</p>
<p><figure><img src="https://s21.ax1x.com/2025/01/11/pEPMB90.jpg" alt="图 3-4: 精度&#x2F;召回权衡：图像根据分类器得分进行排序，高于所选决策阈值的图像被认为是正面的；阈值越高，召回率越低，但（通常）精度越高"><figcaption aria-hidden="true">图 3-4: 精度/召回权衡：图像根据分类器得分进行排序，高于所选决策阈值的图像被认为是正面的；阈值越高，召回率越低，但（通常）精度越高</figcaption></figure></p>
<p>Scikit-Learn 不能让你直接设置阈值，但它可以让你访问它用来做出预测的决策分数。你可以调用分类器的predict() 方法，而不是调用它的 decision_function() 方法，它会为每个实例返回一个分数，然后使用你想要基于这些分数进行预测的任何阈值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;y_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line">&gt;&gt;&gt;y_scores</span><br><span class="line">array([<span class="number">2164.22030239</span>])</span><br><span class="line">&gt;&gt;&gt;threshold = <span class="number">0</span></span><br><span class="line">&gt;&gt;&gt;y_some_digit_pred = (y_scores &gt; threshold) </span><br><span class="line">array([<span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<p>SGDClassifier 使用 0 为阈值，因此前面的代码返回与predict() 方法相同的结果。让我们提高阈值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;threshold = <span class="number">3000</span> </span><br><span class="line">&gt;&gt;&gt;y_some_digit_pred = (y_scores &gt; threshold) </span><br><span class="line">&gt;&gt;&gt;y_some_digit_pred </span><br><span class="line">array([<span class="literal">False</span>])</span><br></pre></td></tr></table></figure>

<p>这证实了提高阈值会降低召回。图像实际上代表一个5，当阈值为0时，分类器会检测到它，但当阈值增加到3000时，它会错过它。如何决定使用哪个阈值？首先，使用cross_val_predict() 函数获取训练集中所有实例的分数，但这次指定要返回决策分数而不是预测分类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>，method=<span class="string">&quot;decision_function&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>有了这些分数，使用 precision_recall_curve() 函数来计算所有可能阈值的精度和召回率（该函数添加的最后精度为0，最后召回率为1，对应于无限阈值），最后，使用Matplotlib绘制精度和召回率作为阈值的函数（图3-5）。让我们显示我们选择的阈值3000：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line"></span><br><span class="line">precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)</span><br><span class="line"></span><br><span class="line">plt.plot(thresholds, precisions[:-<span class="number">1</span>], <span class="string">&quot;b--&quot;</span>, label=<span class="string">&quot;Precision&quot;</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(thresholds, recalls[:-<span class="number">1</span>], <span class="string">&quot;g-&quot;</span>, label=<span class="string">&quot;Recall&quot;</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.vlines(thresholds, <span class="number">0</span>, <span class="number">1.0</span>, <span class="string">&quot;k&quot;</span>, <span class="string">&quot;dotted&quot;</span>, label=<span class="string">&quot;threshold&quot;</span>)</span><br><span class="line">[...] <span class="comment"># beautify the figure: add grid, legend, axis, labels, and circles </span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><figure><img src="https://s21.ax1x.com/2025/01/11/pEPQPbQ.jpg" alt="图 3-5：精确率和召回率对决策阈值的影响"><figcaption aria-hidden="true">图 3-5：精确率和召回率对决策阈值的影响</figcaption></figure></p>
<p>此外该图还呈现了一个有趣的现象，当阈值提高时，准确率偶尔会下降，而召回率一定随阈值提高而单调下降。</p>
<p>在3000这个阈值下，准确率接近90%，召回率约为50%。另一种选择良好的精度&#x2F;召回率权衡的方法是直接绘制精度与召回率的关系，如图3-6所示：</p>
<p><figure><img src="https://s21.ax1x.com/2025/01/11/pEPQkUs.jpg" alt="图 3-6：准确率 vs 召回率"><figcaption aria-hidden="true">图 3-6：准确率 vs 召回率</figcaption></figure></p>
<p>你可以看到，准确度在 80% 的召回率左右开始急剧下降。您可能希望在下降之前选择一个准确率&#x2F;召回率权衡-例如，在60% 左右召回。当然，选择取决于您的项目。假设你的目标是90% 的准确率，你可以用第一张图找到你需要使用的阈值，但这不是很精确。或者，您可以搜索至少提供90%精度的最低阈值。为此，您可以使用 NumPy 数组的 argmax() 方法。这将返回最大值的第一个索引，在本例中意味着第一个 True 值。</p>
<h3 id="受试者操作特征（ROC）曲线"><a href="#受试者操作特征（ROC）曲线" class="headerlink" title="受试者操作特征（ROC）曲线"></a>受试者操作特征（ROC）曲线</h3><p>受试者操作特征（Receiver Operating Characteristic, ROC）曲线是二值分类器的另一个常用工具。它与精确率&#x2F;召回率曲线非常相似，但ROC曲线不是绘制精确率与召回率的关系，而是绘制真阳性率（召回率的另一个名称）与假阳性率（False Positive Rate, FPR）的关系。FPR（也称为fall-out）是指False事件被错误地归类为True事件的比率。它等于1-真阴性率（True Negative Rate, TNR），即正确归类为阴性的阴性实例的比率。TNR也被称为特异性。因此，ROC曲线绘制敏感性（召回率）与1-特异性。为了绘制ROC曲线，首先使用roc_curve() 函数计算各种阈值的 TPR 和 FPR：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>

<p>然后，您可以使用 Matplotlib 绘制FPR与TPR的关系。下面的代码生成图3-7中的图。为了找到对应于90%精度的点，我们需要查找所需阈值的索引：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">idx_for_threshold_at_90 = (thresholds &lt;= threshold_for_90_precision).argmax()</span><br><span class="line">tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]</span><br><span class="line"></span><br><span class="line">plt.plot(fpr, tpr, linewidth=<span class="number">2</span>, label=<span class="string">&quot;ROC curve&quot;</span>) </span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">&#x27;k:&#x27;</span>, label=<span class="string">&quot;Random classifier&#x27;s ROC curve&quot;</span>) </span><br><span class="line">plt.plot([fpr_90], [tpr_90], <span class="string">&quot;ko&quot;</span>, label=<span class="string">&quot;Threshold for 90% precision&quot;</span>) </span><br><span class="line">[...] <span class="comment"># beautify the figure: add labels, grid, legend, arrow, and text </span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><figure><img src="https://s21.ax1x.com/2025/01/13/pEPbgLd.png" alt="图3-7  在所有可能的阈值下绘制假阳性率与真阳性率的ROC曲线；黑色圆圈突出了所选择的比例（90%的准确率和48%的召回率）。"><figcaption aria-hidden="true">图3-7  在所有可能的阈值下绘制假阳性率与真阳性率的ROC曲线；黑色圆圈突出了所选择的比例（90%的准确率和48%的召回率）。</figcaption></figure></p>
<p>这里存在一个权衡：召回率（TPR）越高，分类器产生的假阳性（FPR）越多。虚线表示纯随机分类器的ROC曲线；一个好的分类器会尽可能远离那条线（朝向左上角）。比较分类器的一种方法是测量曲线下面积（Area Under the Curve, AUC）。一个完美的分类器的ROC AUC等于1，而一个纯粹随机的分类器的ROC AUC等于0.5。Scikit-Learn 提供了一个函数来估计ROC AUC：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>roc_auc_score(y_train_5, y_scores)</span><br><span class="line"><span class="number">0.9604938554008616</span></span><br></pre></td></tr></table></figure>

<p>现在让我们创建一个 RandomForestClassifier，我们可以将其PR曲线和${\rm F}_1$分数与 SGDClassifier 进行比较：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier </span><br><span class="line">forest_clf = RandomForestClassifier(random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<p>precision_recall_curve() 函数需要每个实例的标签和分数，因此我们需要训练随机森林分类器并使其为每个实例分配分数。但是 RandomForestClassifier 类没有 decision_function() 方法，因为它的工作方式（我们将在第7章中介绍）。幸运的是，它有一个 predict_proba() 方法，返回每个实例的类概率，我们可以只使用正类的概率作为分数，所以它会工作得很好。我们可以调用 cross_val_predict() 函数来使用交叉验证训练 RandomForestClassifier，并使其预测每个图像的类别概率，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=<span class="number">3</span>, method=<span class="string">&quot;predict_proba&quot;</span>)</span><br><span class="line">&gt;&gt;&gt;y_probas_forest[:<span class="number">2</span>] </span><br><span class="line">array([[<span class="number">0.11</span>, <span class="number">0.89</span>],</span><br><span class="line">    [<span class="number">0.99</span>, <span class="number">0.01</span>]])</span><br></pre></td></tr></table></figure>

<p>让我们看看训练集中前两张图像的类概率。该模型预测第一张图像为正的概率为89%，预测第二张图像为负的概率为99%。因为每个图像要么是正的，要么是负的，每一行的概率加起来是100%。</p>
<p>这些是估计的概率，不是实际的概率。例如，如果你看一下所有被模型分类为阳性的图像，估计概率在50%到60%之间，大约94%的图像实际上是阳性的。因此，在这种情况下，模型估计的概率太低了——但模型也可能过于自信。sklearn calibration package 包含工具来校准估计的概率，使它们更接近实际概率。请参阅本章笔记本中的额外材料部分了解更多细节。</p>
<p>第二列包含正类的估计概率，因此让我们将它们传递给precision_recall_curve()函数，并绘制PR曲线（图3-8）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y_scores_forest = y_probas_forest[:, <span class="number">1</span>] </span><br><span class="line">precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve( y_train_5, y_scores_forest)</span><br><span class="line"></span><br><span class="line">plt.plot(recalls_forest, precisions_forest, <span class="string">&quot;b-&quot;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&quot;Random Forest&quot;</span>)</span><br><span class="line">plt.plot(recalls, precisions, <span class="string">&quot;--&quot;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&quot;SGD&quot;</span>) </span><br><span class="line">[...] <span class="comment"># beautify the figure: add labels, grid, and legend </span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><figure><img src="https://s21.ax1x.com/2025/01/16/pEFyTEQ.png" alt="图 3-8：PR曲线比较：随机森林分类器优于SGD分类器，因为它的PR曲线更接近右上角，并且具有更大的AUC"><figcaption aria-hidden="true">图 3-8：PR曲线比较：随机森林分类器优于SGD分类器，因为它的PR曲线更接近右上角，并且具有更大的AUC</figcaption></figure></p>
<p>如图3-8所示，RandomForestClassifier的PR曲线看起来比SGDClassifier的要好得多：它更接近右上角。其Fi评分和ROC AUC评分也明显较好：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_train_pred_forest =y_probas_forest[:, <span class="number">1</span>]&gt;= <span class="number">0.5</span> <span class="comment"># positive proba 2 50% </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f1_score(y_train_5, y_pred_forest)</span><br><span class="line"><span class="number">0.9242275142688446</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>roc_auc_score(y_train_5, y_scores_forest)</span><br><span class="line"><span class="number">0.9983436731328145</span></span><br></pre></td></tr></table></figure>

<p>试着测量准确率和召回率得分：你应该发现99.1%的准确率和86.6%的召回率。还不错！现在您知道了如何训练二元分类器、为您的任务选择适当的指标、使用交叉验证评估分类器、选择适合您需要的精度&#x2F;召回率权衡，以及使用几个指标和曲线来比较不同的模型。</p>
<h2 id="多元分类"><a href="#多元分类" class="headerlink" title="多元分类"></a>多元分类</h2><p>二元分类器区分两个类，而多类分类器（也称为多项分类器）可以区分两个以上的类。一些 Scikit-Learn 分类器（例如 LogisticRegression，RandomForestClassifier 和 GaussianNB）能够本地处理多个类。其他的则是严格的二元分类器（例如 SGDClassifier 和 SVC）。但是，您可以使用各种策略来使用多个二进制分类器执行多类分类。</p>
<p>OvR策略：创建可以将数字图像分类为10类（从0到9）的系统的一种方法是训练10个二进制分类器，每个数字一个（0检测器、1检测器、2检测器等等）。然后，当你想对图像进行分类时，你从该图像的每个分类器中获得决策分数，然后<strong>选择分类器输出最高分数的类</strong>。这被称为一对其余（one-versus-the-rest, OvR）策略，有时也称为一对所有（one-versus-all, OvA）策略。</p>
<p>OvO策略：另一种策略是为每一对数字训练一个二元分类器：一个用于区分0和1，另一个用于区分0和2，另一个用于区分1和2，以此类推。这被称为1对1（one-versus-one, OvO）策略。如果有N个类，你需要训练 Nx(N-1)&#x2F;2 个分类器。对于 MNIST 问题，这意味着要训练 45 个二元分类器！当您想要对图像进行分类时，您必须在所有 45 个分类器中运行图像，并<strong>查看哪个类赢得最多的决斗</strong>。OvO 的主要优点是每个分类器只需要在包含它必须区分的两个类的训练集部分进行训练。</p>
<p>一些算法（如支持向量机分类器）随着训练集的大小缩放得很差。对于这些算法，OvO是首选，因为在小的训练集上训练许多分类器比在大的训练集上训练很少的分类器要快。然而，对于大多数二进制分类算法，OvR 是首选。当您尝试使用二进制分类算法时，Scikit-Learn 会检测一个多类分类任务，它会根据算法自动运行 OvR 或 OvO。让我们使用 sklearn.svm.SVC 类（参见第5章）来尝试使用支持向量机分类器。我们只训练前 2000 张图像，否则将花费很长时间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">svm_clf = SVC(random_state=<span class="number">42</span>) </span><br><span class="line">svm_clf.fit(X_train[:<span class="number">2000</span>], y_train[:<span class="number">2000</span>]) <span class="comment"># y_train, not y_train_5</span></span><br></pre></td></tr></table></figure>

<p>这很简单！我们使用从0到9的原始目标类（y_train）来训练SVC，而不是使用5对其余目标类（y_train_5）。由于有10个类（即超过2个），Scikit-Learn使用OvO策略并训练了45个二元分类器。现在让我们对图像做一个预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;svm_clf.predict([some_digit]) </span><br><span class="line">array([<span class="string">&#x27;5&#x27;</span>], dtype=<span class="built_in">object</span>)</span><br></pre></td></tr></table></figure>

<p>这是正确的!这段代码实际上做了45个预测，每对类一个，它选择了赢得最多决斗的类。如果调用decision_function() 方法，您将看到它为每个实例返回10个分数：每个类一个。每个类的得分等于赢得决斗的次数，加上或减去一个小调整（最大±0.33），以打破平局，基于分类器得分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;some_digit_scores=svm_clf.decision_function([some_digit])</span><br><span class="line">&gt;&gt;&gt;some_digit_scores.<span class="built_in">round</span>(<span class="number">2</span>)</span><br><span class="line">array([[ <span class="number">3.79</span>, <span class="number">0.73</span>, <span class="number">6.06</span>, <span class="number">8.3</span>,-<span class="number">0.29</span>, <span class="number">9.3</span>, <span class="number">1.75</span>, <span class="number">2.77</span>, <span class="number">7.21</span>, <span class="number">4.82</span>]])</span><br></pre></td></tr></table></figure>

<p>最高分9.3分，确实是类别5所对应的分数。</p>
<p>当一个分类器被训练时，它将目标类的列表存储在其classes属性中，按值排序。在MNIST的情况下，classes_ 数组中每个类的索引方便地匹配类本身（例如，索引5处的类恰好是类‘5’），但通常你不会那么幸运；你需要像这样查找类标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>svm_clf.classes_</span><br><span class="line">array(ГO<span class="string">&#x27;, &#x27;</span><span class="number">1</span><span class="string">&#x27;, &#x27;</span><span class="number">2</span><span class="string">&#x27;, &#x27;</span><span class="number">3</span><span class="string">&#x27;, &#x27;</span><span class="number">4</span><span class="string">&#x27;, &#x27;</span><span class="number">5</span><span class="string">&#x27;, &#x27;</span><span class="number">6</span><span class="string">&#x27;, &#x27;</span><span class="number">7</span><span class="string">&#x27;, &#x27;</span><span class="number">8</span><span class="string">&#x27;, &#x27;</span><span class="number">9</span><span class="string">&#x27;], dtype=object)</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; svm_clf.classes_[class_id] </span></span><br><span class="line"><span class="string">&#x27;</span><span class="number">5</span><span class="string">&#x27;</span></span><br></pre></td></tr></table></figure>

<p>如果你想强迫 Scikit-Learn 使用OvO或OvR的测试，你可以使用 onevsonecclassifier 或 OneVsRestClassifier 类。只需创建一个实例并将一个分类器传递给它的构造函数（它甚至不必是二进制分类器）。例如，以下代码使用基于 SVC 的 OvR 策略创建了一个多类分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"></span><br><span class="line">ovr_clf = OneVsRestClassifier(SVC(random_state=<span class="number">42</span>)) </span><br><span class="line">ovr_clf.fit(X_train[:<span class="number">2000</span>], y_train[:<span class="number">2000</span>])</span><br></pre></td></tr></table></figure>

<p>让我们做一个预测，并检查训练的分类器的数量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>ovr_clf.predict([some_digit]) </span><br><span class="line">array([<span class="string">&#x27;5&#x27;</span>], dtype=<span class="string">&#x27;&lt;U1&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(ovr_clf.estimators_)</span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure>

<p>在多类数据集上训练 SGDClassifier 并使用它进行预测也同样简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sgd_clf = SGDClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">&gt;&gt;&gt;sgd_clf.fit(X_train, y_train)</span><br><span class="line">&gt;&gt;&gt;sgd_clf.predict([some_digit])</span><br><span class="line">array([<span class="string">&#x27;3&#x27;</span>], dtype=<span class="string">&#x27;&lt;U1&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>哎呀，这是不正确的。预测错误确实会发生！这次 Scikit-Learn 在底层使用了OvR策略：因为有10个类，所以它训练了10个二元分类器。decision_function()方法现在每个类返回一个值。让我们看看 SGD 分类器分配给每个类的分数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.decision_function([some_digit]).<span class="built_in">round</span>()</span><br><span class="line">array([[-<span class="number">31893.</span>, -<span class="number">34420.</span>, -<span class="number">9531.</span>, <span class="number">1824.</span>, -<span class="number">22320.</span>, -<span class="number">1386.</span>, -<span class="number">26189.</span>,-<span class="number">16148.</span>, -<span class="number">4604.</span>,-<span class="number">12051.</span>]])</span><br></pre></td></tr></table></figure>

<p>你可以看到分类器对它的预测不是很有信心：几乎所有的分数都是非常负的，而类3的分数是+1824，类5的分数是-1386，并没有落后太多。当然，您需要在多个图像上评估这个分类器。由于每个类中有大致相同数量的图像，因此精度度量很好。像往常一样，你可以使用 cross_val_score() 函数来评估模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(sgd_clf, X_train, y_train, cv=<span class="number">3</span>, scoring=<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">array([<span class="number">0.87365</span>, <span class="number">0.85835</span>, <span class="number">0.8689</span> ])</span><br></pre></td></tr></table></figure>

<p>它在所有测试折叠中都超过了85.8%。如果你使用随机分类器，你会得到10%的准确率，所以这不是一个糟糕的分数，但你仍然可以做得更好。简单地缩放输入（如第2章所述）将精度提高到89.1%以上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler = StandardScaler()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_scaled = scaler.fit_transform(X_train.astype(<span class="string">&quot;float64&quot;</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>, scoring=<span class="string">&quot;accuracy&quot;</span>) </span><br><span class="line">array([<span class="number">0.8983</span>, <span class="number">0.891</span> , <span class="number">0.9018</span>])</span><br></pre></td></tr></table></figure>

<h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2>
    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Hai-Wei Chai (柴海伟)
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://hwchai.com/Hands-On_ML_Sec3/" title="《Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow, Third Edition》 全书第三章：分类">https://hwchai.com/Hands-On_ML_Sec3/</a>
  </li>
  <li class="post-copyright-license">
      <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Artificial-Intelligence/" rel="tag"># Artificial Intelligence</a>
              <a href="/tags/Pytorch/" rel="tag"># Pytorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/AI-Multi-head/" rel="prev" title="人工智能：多头自注意力（Multi-Head Attention）机制">
                  <i class="fa fa-angle-left"></i> 人工智能：多头自注意力（Multi-Head Attention）机制
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/Exp012-book02/" rel="next" title="另一种选择">
                  另一种选择 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-user"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Hai-Wei Chai (柴海伟)</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"https://vercel.hwchai.com/","cssUrl":"https://hwchai.com/download/Waline.css","commentCount":true,"pageview":false,"locale":{"placeholder":"请留下邮箱，若有回复您将收到提醒。QQ邮箱可以自动识别头像喔~"},"pageSize":10,"visitor":false,"comment_count":true,"meta":["nick","mail","link"],"requiredMeta":["nick"],"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","emoji":["https://unpkg.com/@waline/emojis@1.0.1/bilibili"],"login":"disable","el":"#waline","comment":true,"path":"/Hands-On_ML_Sec3/"}</script>
<link rel="stylesheet" href="https://hwchai.com/download/Waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
